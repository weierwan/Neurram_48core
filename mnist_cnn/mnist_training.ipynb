{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Set the matplotlib default settings\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..\\\\cifar10_resnet')\n",
    "from layer_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_3bit = quantize_rescale(quantize_unsigned(x_train, 3, 1.0), 3, 1.0)\n",
    "x_test_3bit = quantize_rescale(quantize_unsigned(x_test, 3, 1.0), 3, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 10000\n",
    "total_index = np.arange(x_train.shape[0])\n",
    "val_index = np.random.choice(total_index, split, replace=False)\n",
    "train_index = np.delete(total_index, val_index)\n",
    "\n",
    "x_val_div = x_train[val_index]\n",
    "x_train_div = x_train[train_index]\n",
    "y_val_div = y_train[val_index]\n",
    "y_train_div = y_train[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_noise (conv2d_noise)  (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_noise_1 (conv2d_noise (None, 28, 28, 8)         584       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_noise_2 (conv2d_noise (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_noise_3 (conv2d_noise (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_noise_4 (conv2d_noise (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_noise_5 (conv2d_noise (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (dense_noise)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 23,170\n",
      "Trainable params: 23,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weight_noise_train = 0.0\n",
    "weight_noise_test = 0.0\n",
    "weight_bits = None\n",
    "activation_bits = 3\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv2d_noise(8, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, num_bits=weight_bits, input_shape=input_shape))\n",
    "model.add(activation_quant(activation_bits, 3))\n",
    "# model.add(layers.Activation(activations.relu))\n",
    "model.add(conv2d_noise(8, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, num_bits=weight_bits))\n",
    "model.add(activation_quant(activation_bits, 3))\n",
    "# model.add(layers.Activation(activations.relu))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(conv2d_noise(16, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, num_bits=weight_bits))\n",
    "model.add(activation_quant(activation_bits, 3))\n",
    "# model.add(layers.Activation(activations.relu))\n",
    "model.add(conv2d_noise(16, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, num_bits=weight_bits))\n",
    "model.add(activation_quant(activation_bits, 3))\n",
    "# model.add(layers.Activation(activations.relu))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(conv2d_noise(32, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, num_bits=weight_bits))\n",
    "model.add(activation_quant(activation_bits, 3))\n",
    "# model.add(layers.Activation(activations.relu))\n",
    "model.add(conv2d_noise(32, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, num_bits=weight_bits))\n",
    "model.add(activation_quant(activation_bits, 3))\n",
    "# model.add(layers.Activation(activations.relu))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(dense_noise(num_classes, activation='softmax', noise_train=weight_noise_train, noise_test=weight_noise_test, num_bits=weight_bits, name='dense'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              metrics=['accuracy'])\n",
    "model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A shallow version\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv2d_noise(8, kernel_size=(3, 3), activation='relu', noise_magnitude=0.1, name='conv2d', input_shape=input_shape))\n",
    "model.add(activation_quant(3, 3.7))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Conv2D(12, (3, 3), padding='valid', activation='relu', name='conv2d_1'))\n",
    "model.add(conv2d_noise(12, (3, 3), padding='valid', activation='relu', noise_magnitude=0.1, name='conv2d_1'))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(activation_quant(3, 10.0))\n",
    "# model.add(Dense(num_classes, activation='softmax', name='dense'))\n",
    "model.add(dense_noise(num_classes, activation='softmax', noise_magnitude=0.1, name='dense'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.train.AdamOptimizer(2e-4),\n",
    "              metrics=['accuracy'])\n",
    "model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9993 ETA: 0s - loss: 0.0018 - accuracy: 0.99\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.99000, saving model to mnist_cnn_7layer_input_64bit_wnoise0.00_val\\\n",
      "50000/50000 [==============================] - 20s 394us/sample - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0648 - val_accuracy: 0.9900\n",
      "Epoch 2/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 00002: val_accuracy did not improve from 0.99000\n",
      "50000/50000 [==============================] - 18s 358us/sample - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0656 - val_accuracy: 0.9895\n",
      "Epoch 3/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9983\n",
      "Epoch 00003: val_accuracy improved from 0.99000 to 0.99050, saving model to mnist_cnn_7layer_input_64bit_wnoise0.00_val\\\n",
      "50000/50000 [==============================] - 18s 353us/sample - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0542 - val_accuracy: 0.9905\n",
      "Epoch 4/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9990\n",
      "Epoch 00004: val_accuracy did not improve from 0.99050\n",
      "50000/50000 [==============================] - 17s 346us/sample - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0642 - val_accuracy: 0.9882\n",
      "Epoch 5/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9990\n",
      "Epoch 00005: val_accuracy did not improve from 0.99050\n",
      "50000/50000 [==============================] - 18s 354us/sample - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0566 - val_accuracy: 0.9902\n",
      "Epoch 6/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 00006: val_accuracy did not improve from 0.99050\n",
      "50000/50000 [==============================] - 17s 348us/sample - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0627 - val_accuracy: 0.9903\n",
      "Epoch 7/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9991\n",
      "Epoch 00007: val_accuracy did not improve from 0.99050\n",
      "50000/50000 [==============================] - 18s 350us/sample - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0708 - val_accuracy: 0.9894\n",
      "Epoch 8/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9987\n",
      "Epoch 00008: val_accuracy did not improve from 0.99050\n",
      "50000/50000 [==============================] - 17s 349us/sample - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0647 - val_accuracy: 0.9897\n",
      "Epoch 9/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9994\n",
      "Epoch 00009: val_accuracy did not improve from 0.99050\n",
      "50000/50000 [==============================] - 17s 343us/sample - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0639 - val_accuracy: 0.9904\n",
      "Epoch 10/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9986\n",
      "Epoch 00010: val_accuracy improved from 0.99050 to 0.99090, saving model to mnist_cnn_7layer_input_64bit_wnoise0.00_val\\\n",
      "50000/50000 [==============================] - 17s 348us/sample - loss: 0.0034 - accuracy: 0.9986 - val_loss: 0.0551 - val_accuracy: 0.9909\n",
      "Epoch 11/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9990\n",
      "Epoch 00011: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 349us/sample - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.0675 - val_accuracy: 0.9901\n",
      "Epoch 12/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 00012: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 363us/sample - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0652 - val_accuracy: 0.9899\n",
      "Epoch 13/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9987\n",
      "Epoch 00013: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 352us/sample - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0692 - val_accuracy: 0.9885\n",
      "Epoch 14/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 00014: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 19s 372us/sample - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0644 - val_accuracy: 0.9899\n",
      "Epoch 15/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9988\n",
      "Epoch 00015: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 355us/sample - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0550 - val_accuracy: 0.9891\n",
      "Epoch 16/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9990\n",
      "Epoch 00016: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 363us/sample - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0536 - val_accuracy: 0.9905\n",
      "Epoch 17/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 00017: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 343us/sample - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0594 - val_accuracy: 0.9898\n",
      "Epoch 18/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 00018: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 19s 386us/sample - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0654 - val_accuracy: 0.9904\n",
      "Epoch 19/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 1.7662e-04 - accuracy: 0.9999\n",
      "Epoch 00019: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 20s 400us/sample - loss: 1.7594e-04 - accuracy: 0.9999 - val_loss: 0.0694 - val_accuracy: 0.9902\n",
      "Epoch 20/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 00020: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 358us/sample - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0681 - val_accuracy: 0.9888\n",
      "Epoch 21/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9991\n",
      "Epoch 00021: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 355us/sample - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0570 - val_accuracy: 0.9891\n",
      "Epoch 22/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9995\n",
      "Epoch 00022: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 333us/sample - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0752 - val_accuracy: 0.9892\n",
      "Epoch 23/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9991\n",
      "Epoch 00023: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 345us/sample - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0608 - val_accuracy: 0.9907\n",
      "Epoch 24/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 00024: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 369us/sample - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0714 - val_accuracy: 0.9874\n",
      "Epoch 25/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9989\n",
      "Epoch 00025: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 19s 372us/sample - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0575 - val_accuracy: 0.9899\n",
      "Epoch 26/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993\n",
      "Epoch 00026: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 366us/sample - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0834 - val_accuracy: 0.9889\n",
      "Epoch 27/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9991\n",
      "Epoch 00027: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 349us/sample - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0765 - val_accuracy: 0.9893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 00028: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 342us/sample - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0789 - val_accuracy: 0.9898\n",
      "Epoch 29/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 00029: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 366us/sample - loss: 0.0028 - accuracy: 0.9989 - val_loss: 0.0827 - val_accuracy: 0.9893\n",
      "Epoch 30/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9986\n",
      "Epoch 00030: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 339us/sample - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0751 - val_accuracy: 0.9902\n",
      "Epoch 31/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00031: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 18s 351us/sample - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0599 - val_accuracy: 0.9909\n",
      "Epoch 32/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 00032: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 347us/sample - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0619 - val_accuracy: 0.9909\n",
      "Epoch 33/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9988\n",
      "Epoch 00033: val_accuracy did not improve from 0.99090\n",
      "50000/50000 [==============================] - 17s 342us/sample - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0731 - val_accuracy: 0.9897\n",
      "Epoch 34/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9993\n",
      "Epoch 00034: val_accuracy improved from 0.99090 to 0.99140, saving model to mnist_cnn_7layer_input_64bit_wnoise0.00_val\\\n",
      "50000/50000 [==============================] - 17s 332us/sample - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0691 - val_accuracy: 0.9914\n",
      "Epoch 35/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9994\n",
      "Epoch 00035: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 17s 338us/sample - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0707 - val_accuracy: 0.9900\n",
      "Epoch 36/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9991\n",
      "Epoch 00036: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 17s 343us/sample - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0759 - val_accuracy: 0.9900\n",
      "Epoch 37/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 00037: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 18s 351us/sample - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0783 - val_accuracy: 0.9900\n",
      "Epoch 38/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9992\n",
      "Epoch 00038: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 18s 351us/sample - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.0723 - val_accuracy: 0.9896\n",
      "Epoch 39/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 00039: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 17s 347us/sample - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0899 - val_accuracy: 0.9885\n",
      "Epoch 40/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 00040: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 17s 350us/sample - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0700 - val_accuracy: 0.9906\n",
      "Epoch 41/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 2.4810e-04 - accuracy: 0.9999\n",
      "Epoch 00041: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 18s 357us/sample - loss: 2.4707e-04 - accuracy: 0.9999 - val_loss: 0.1017 - val_accuracy: 0.9892\n",
      "Epoch 42/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 00042: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 18s 364us/sample - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0909 - val_accuracy: 0.9885\n",
      "Epoch 43/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9987\n",
      "Epoch 00043: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 17s 341us/sample - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0786 - val_accuracy: 0.9900\n",
      "Epoch 44/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9992\n",
      "Epoch 00044: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 19s 373us/sample - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0713 - val_accuracy: 0.9908\n",
      "Epoch 45/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4043e-04 - accuracy: 1.0000\n",
      "Epoch 00045: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 20s 395us/sample - loss: 1.4021e-04 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9906\n",
      "Epoch 46/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 3.6189e-05 - accuracy: 1.0000\n",
      "Epoch 00046: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 20s 393us/sample - loss: 3.6092e-05 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 0.9906\n",
      "Epoch 47/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00047: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 18s 362us/sample - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0819 - val_accuracy: 0.9898\n",
      "Epoch 48/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9982\n",
      "Epoch 00048: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 17s 343us/sample - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0766 - val_accuracy: 0.9888\n",
      "Epoch 49/50\n",
      "49792/50000 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9992\n",
      "Epoch 00049: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 18s 357us/sample - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0700 - val_accuracy: 0.9908\n",
      "Epoch 50/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 8.1334e-04 - accuracy: 0.9997\n",
      "Epoch 00050: val_accuracy did not improve from 0.99140\n",
      "50000/50000 [==============================] - 20s 402us/sample - loss: 8.1211e-04 - accuracy: 0.9997 - val_loss: 0.0865 - val_accuracy: 0.9903\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "save_dir = 'mnist_cnn_7layer_input_64bit_wnoise0.00_val\\\\'\n",
    "# load_dir = save_dir\n",
    "# model.load_weights(load_dir)\n",
    "\n",
    "ckpt_cbk = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=save_dir,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1)\n",
    "\n",
    "model.fit(x_train_div, y_train_div,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[ckpt_cbk],\n",
    "          verbose=1,\n",
    "          validation_data=(x_val_div, y_val_div))\n",
    "\n",
    "model.save_weights(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "for layer in model.layers:\n",
    "    weights[layer.name] = layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for klayer in model.layers:\n",
    "    if klayer.name in weights:\n",
    "        layer_params = weights[klayer.name]\n",
    "        if weight_bits is not None and ('conv2d' in klayer.name or 'dense' in klayer.name):\n",
    "            weight_range = np.abs(layer_params[0]).max()\n",
    "            bias_range = np.abs(layer_params[1]).max()\n",
    "            layer_params = layer_params + [weight_range, bias_range]\n",
    "        klayer.set_weights(layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1a98ccec0b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = 'mnist_cnn_7layer_input_3bit_wnoise0.15\\\\checkpoint'\n",
    "load_dir = save_dir\n",
    "model.load_weights(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04687360113807137\n",
      "Test accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0298 - accuracy: 0.9912\n",
      "10000/10000 [==============================] - 2s 231us/sample - loss: 0.0325 - accuracy: 0.9912\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 0.0323 - accuracy: 0.9908\n",
      "10000/10000 [==============================] - 2s 230us/sample - loss: 0.0305 - accuracy: 0.9913\n",
      "10000/10000 [==============================] - 2s 232us/sample - loss: 0.0306 - accuracy: 0.9905\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0302 - accuracy: 0.9927\n",
      "10000/10000 [==============================] - 2s 231us/sample - loss: 0.0313 - accuracy: 0.9915\n",
      "10000/10000 [==============================] - 2s 244us/sample - loss: 0.0308 - accuracy: 0.9918\n",
      "10000/10000 [==============================] - 2s 238us/sample - loss: 0.0313 - accuracy: 0.9923\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0320 - accuracy: 0.9911\n",
      "0.9914399921894074\n"
     ]
    }
   ],
   "source": [
    "ITERATION = 10\n",
    "accuracy = np.zeros(ITERATION)\n",
    "for i in range(ITERATION):\n",
    "    accuracy[i] = model.evaluate(x_test_3bit, y_test, verbose=1)[1]\n",
    "    \n",
    "print(accuracy.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
