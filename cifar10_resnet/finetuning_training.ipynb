{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, Iterator\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import BatchNormalization, AveragePooling2D, Input, Flatten, Dropout\n",
    "from layer_utils import activation_quant, conv2d_noise, dense_noise, noise_injection\n",
    "from layers_numpy import quantize_unsigned, quantize_rescale\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Set the matplotlib default settings\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
    "epochs = 20\n",
    "weight_noise_train = 0.20\n",
    "weight_noise_test = 0.0\n",
    "activation_bits = 3\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ResNet20v1_filter16_act3b_wnoise0.20_input0.95\n",
      "Finetuning input from conv2d_noise_4\n"
     ]
    }
   ],
   "source": [
    "num_segment_previous = 1\n",
    "num_segment_current = 1\n",
    "\n",
    "model_name = 'filter16_act%db_wnoise%.2f_input0.95' % (activation_bits, weight_noise_train)\n",
    "model_type = 'ResNet%dv%d_%s' % (20, 1, model_name)\n",
    "orig_model_name = 'fwd_finetune_all_val'  # 'cifar10_%s_model' % model_type\n",
    "\n",
    "print('Model name: %s' % model_type)\n",
    "\n",
    "finetune_previous_layer = 'add_7'\n",
    "finetune_current_layer = 'conv2d_noise_20'\n",
    "print('Finetuning input from %s' % finetune_current_layer)\n",
    "ft_model_name = 'fwd_finetune_%s_relu_decay_1e-3_3' % finetune_current_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet20_finetune(activation_bits, weight_noise_train, weight_noise_test, relu_decay, trainable_relu=True):\n",
    "    # Build model for finetuning\n",
    "\n",
    "    layer_input_0 = Input(shape=(8, 8, 64)) # a new input tensor to be able to feed the desired layer\n",
    "    layer_input_1 = Input(shape=(8, 8, 64))\n",
    "    \n",
    "    y = noise_injection(0.0)(layer_input_0)\n",
    "    x = noise_injection(0.0)(layer_input_1)\n",
    "\n",
    "#     create the new nodes for each layer in the path\n",
    "\n",
    "#     y = conv2d_noise(16, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise')(y)\n",
    "#     y = BatchNormalization(name='batch_normalization')(y)\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, name='activation_quant')(y)\n",
    "\n",
    "    # Stack 1, block 1\n",
    "#     x = conv2d_noise(16, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_1')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_1')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, name='activation_quant_1')(x)\n",
    "#     x = conv2d_noise(16, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_2')(x)\n",
    "#     x = BatchNormalization(name='batch_normalization_2')(x)\n",
    "#     y = keras.layers.add([y, x])\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, name='activation_quant_2')(y)\n",
    "\n",
    "    # Stack 1, block 2\n",
    "#     x = conv2d_noise(16, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_3')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_3')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, name='activation_quant_3')(x)\n",
    "#     x = conv2d_noise(16, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_4')(x)\n",
    "#     x = BatchNormalization(name='batch_normalization_4')(x)\n",
    "#     y = keras.layers.add([y, x])\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, name='activation_quant_4')(y)\n",
    "\n",
    "    # Stack 1, block 3\n",
    "#     x = conv2d_noise(16, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_5')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_5')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, name='activation_quant_5')(x)\n",
    "#     x = conv2d_noise(16, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_6')(x)\n",
    "#     x = BatchNormalization(name='batch_normalization_6')(x)\n",
    "#     y = keras.layers.add([y, x])\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, name='activation_quant_6')(y)\n",
    "\n",
    "    # Stack 2, block 1\n",
    "#     x = conv2d_noise(32, strides=2, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_7')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_7')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, name='activation_quant_7')(x)\n",
    "#     x = conv2d_noise(32, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_8')(x)\n",
    "#     y = conv2d_noise(32, kernel_size=1, strides=2, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_9')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_8')(x)\n",
    "#     y = keras.layers.add([y, x])\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, name='activation_quant_8')(y)\n",
    "\n",
    "    # Stack 2, block 2\n",
    "#     x = conv2d_noise(32, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_10')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_9')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, name='activation_quant_9')(x)\n",
    "#     x = conv2d_noise(32, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_11')(x)\n",
    "#     x = BatchNormalization(name='batch_normalization_10')(x)\n",
    "#     y = keras.layers.add([y, x])\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_10')(y)\n",
    "\n",
    "    # Stack 2, block 3\n",
    "#     x = conv2d_noise(32, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_12')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_11')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_11')(x)\n",
    "#     x = conv2d_noise(32, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_13')(x)\n",
    "#     x = BatchNormalization(name='batch_normalization_12')(x)\n",
    "#     y = keras.layers.add([y, x])\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_12')(y)\n",
    "\n",
    "    # Stack 3, block 1\n",
    "#     x = conv2d_noise(64, strides=2, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_14')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_13')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_13')(x)\n",
    "#     x = conv2d_noise(64, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_15')(x)\n",
    "#     y = conv2d_noise(64, kernel_size=1, strides=2, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_16')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_14')(x)\n",
    "#     y = keras.layers.add([y, x])\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_14')(y)\n",
    "\n",
    "    # Stack 3, block 2\n",
    "#     x = conv2d_noise(64, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_17')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_15')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_15')(x)\n",
    "#     x = conv2d_noise(64, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_18')(x)\n",
    "#     x = BatchNormalization(name='batch_normalization_16')(x)\n",
    "#     y = keras.layers.add([y, x])\n",
    "#     y = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_16')(y)\n",
    "\n",
    "    # Stack 3, block 3\n",
    "#     x = conv2d_noise(64, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_19')(y)\n",
    "#     x = BatchNormalization(name='batch_normalization_17')(x)\n",
    "#     x = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_17')(x)\n",
    "#     x = conv2d_noise(64, strides=1, padding='same', noise_train=weight_noise_train, noise_test=weight_noise_test, name='conv2d_noise_20')(x)\n",
    "#     x = BatchNormalization(name='batch_normalization_18')(x)\n",
    "    y = keras.layers.add([y, x])\n",
    "\n",
    "    y = AveragePooling2D(pool_size=8)(y)\n",
    "    y = Flatten()(y)\n",
    "    y = activation_quant(num_bits=activation_bits, max_value=3, decay=relu_decay, trainable=trainable_relu, name='activation_quant_18')(y)\n",
    "\n",
    "    outputs = dense_noise(10, activation='softmax', noise_train=weight_noise_train, noise_test=weight_noise_test, name='dense_noise')(y)\n",
    "\n",
    "    # create the model\n",
    "    model = Model([layer_input_0, layer_input_1], outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(1e-5),\n",
    "                  metrics=['accuracy'])\n",
    "    # \tmodel.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(activation_bits, weight_noise_train, weight_noise_test, relu_decay, trainable_relu, weights_map):\n",
    "    K.clear_session()\n",
    "    model = resnet20_finetune(activation_bits, weight_noise_train, weight_noise_test, relu_decay, trainable_relu=trainable_relu)\n",
    "    for klayer in model.layers:\n",
    "        if klayer.name in weights_map:\n",
    "            klayer.set_weights(weights_map[klayer.name])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load original model weights\n",
    "ckpt_dir = os.path.join(os.getcwd(), model_type)\n",
    "orig_path = os.path.join(ckpt_dir, orig_model_name)\n",
    "orig_model = load_model(orig_path)\n",
    "weights = {}\n",
    "for olayer in orig_model.layers:\n",
    "    weights[olayer.name] = olayer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finetuning inputs\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "DATA_PATH = '/scratch/users/weierwan/finetune_all/'\n",
    "# train_current_inputs = 0\n",
    "# test_current_inputs = 0\n",
    "# for i in range(2):\n",
    "#     tmp = np.load(os.path.join(DATA_PATH, '%s_%d_train.npz' % (finetune_previous_layer, i)))\n",
    "#     train_previous_inputs += (tmp['out_chip'] - tmp['intercept']) / tmp['slope']\n",
    "#     tmp = np.load(os.path.join(DATA_PATH, '%s_%d_test.npz' % (finetune_current_layer, i)))\n",
    "#     test_current_inputs += (tmp['out_chip'] - tmp['intercept']) / tmp['slope']\n",
    "# train_current_inputs = 0\n",
    "# test_current_inputs = 0\n",
    "# for i in range(num_segment_current):\n",
    "# \ttmp = np.load(os.path.join(DATA_PATH, '%s_%d_train.npz' % (finetune_current_layer, i)))\n",
    "# \ttrain_current_inputs += (tmp['out_chip'] - tmp['intercept']) / tmp['slope']\n",
    "# \ttmp = np.load(os.path.join(DATA_PATH, '%s_%d_test.npz' % (finetune_current_layer, i)))\n",
    "# \ttest_current_inputs += (tmp['out_chip'] - tmp['intercept']) / tmp['slope']\n",
    "\n",
    "train_previous_inputs = np.load(os.path.join(DATA_PATH, '%s_train_pact.npy' % finetune_previous_layer))\n",
    "test_previous_inputs = np.load(os.path.join(DATA_PATH, '%s.npy' % finetune_previous_layer))\n",
    "\n",
    "# tmp = np.load(os.path.join(DATA_PATH, '%s.npz' % finetune_previous_layer))\n",
    "# test_previous_inputs = tmp['input_sample_test']\n",
    "# train_previous_inputs = tmp['input_sample_train']\n",
    "# relux = tmp['relux']\n",
    "\n",
    "train_current_inputs = np.load(os.path.join(DATA_PATH, '%s_train_pact.npy' % finetune_current_layer))\n",
    "test_current_inputs = np.load(os.path.join(DATA_PATH, '%s.npy' % finetune_current_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHdCAYAAAAJn+38AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEy/5O80zycGlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_previous_inputs[:1000].flatten(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(weights['activation_quant_16'][0])\n",
    "print(relux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_previous_inputs = quantize_rescale(train_previous_inputs, 3, relux)\n",
    "test_previous_inputs = quantize_rescale(test_previous_inputs, 3, relux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 10000\n",
    "total_index = np.arange(50000)\n",
    "# val_index = np.random.choice(total_index, split, replace=False)\n",
    "val_index = np.load('val_index.npy')\n",
    "train_index = np.delete(total_index, val_index)\n",
    "\n",
    "val_previous_inputs_div = train_previous_inputs[val_index]\n",
    "train_previous_inputs_div = train_previous_inputs[train_index]\n",
    "val_current_inputs_div = train_current_inputs[val_index]\n",
    "train_current_inputs_div = train_current_inputs[train_index]\n",
    "y_val_div = y_train[val_index]\n",
    "y_train_div = y_train[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('val_index', val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 65us/sample - loss: 0.5399 - accuracy: 0.8393\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model before the fine-tuning (without noise injection)\n",
    "\n",
    "model = build_model(activation_bits, weight_noise_train, weight_noise_test, 1e-3, True, weights)\n",
    "accuracy_train_nf = model.evaluate([train_previous_inputs, train_current_inputs], y_train, verbose=1)[1]\n",
    "accuracy_test_nf = model.evaluate([test_previous_inputs, test_current_inputs], y_test, verbose=1)[1]\n",
    "accuracy_val_nf = model.evaluate([val_previous_inputs_div, val_current_inputs_div], y_val_div, verbose=1)[1]\n",
    "\n",
    "print('Noise-free train accuracy:', accuracy_train_nf)\n",
    "print('Noise-free test accuracy:', accuracy_test_nf)\n",
    "print('noisy train accuracy:', accuracy_train.mean())\n",
    "print('noisy test accuracy:', accuracy_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 68us/sample - loss: 0.5428 - accuracy: 0.8390\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.5408 - accuracy: 0.8392\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.5425 - accuracy: 0.8388\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.5405 - accuracy: 0.8386\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.5415 - accuracy: 0.8383\n",
      "10000/10000 [==============================] - 1s 51us/sample - loss: 0.5428 - accuracy: 0.8388\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.5416 - accuracy: 0.8383\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.5416 - accuracy: 0.8383\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.5423 - accuracy: 0.8386\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.5417 - accuracy: 0.8379\n",
      "0.8385799944400787\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model before the fine-tuning (with noise injection)\n",
    "model = build_model(activation_bits, weight_noise_train, 0.1, 1e-3, True, weights)\n",
    "accuracy = np.zeros(10)\n",
    "for i in range(10):\n",
    "    accuracy[i] = model.evaluate([test_previous_inputs, test_current_inputs], y_test, verbose=1)[1]\n",
    "print(accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model(activation_bits, weight_noise_train, weight_noise_test, 1e-3, True, weights)\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('activation_quant_16').output)\n",
    "test_previous_inputs = intermediate_layer_model.predict([test_previous_inputs, test_current_inputs])\n",
    "train_previous_inputs = intermediate_layer_model.predict([train_previous_inputs, train_current_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relux = weights['activation_quant_16'][0]\n",
    "input_sample_test = quantize_unsigned(test_previous_inputs, 3, relux)\n",
    "input_sample_train = quantize_unsigned(train_previous_inputs, 3, relux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_PATH, 'activation_quant_16_test_pact'), test_previous_inputs)\n",
    "np.save(os.path.join(DATA_PATH, 'activation_quant_16_train_pact'), train_previous_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 2s 36us/sample - loss: 0.1138 - accuracy: 0.9615 - val_loss: 0.5243 - val_accuracy: 0.8559\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 1s 27us/sample - loss: 0.1146 - accuracy: 0.9627 - val_loss: 0.5245 - val_accuracy: 0.8563\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 1s 26us/sample - loss: 0.1140 - accuracy: 0.9628 - val_loss: 0.5249 - val_accuracy: 0.8564\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 1s 27us/sample - loss: 0.1145 - accuracy: 0.9622 - val_loss: 0.5253 - val_accuracy: 0.8562\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 1s 27us/sample - loss: 0.1140 - accuracy: 0.9623 - val_loss: 0.5255 - val_accuracy: 0.8562\n",
      "INFO:tensorflow:Assets written to: /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_conv2d_noise_20_relu_decay_1e-3_2_final/assets\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune\n",
    "# model = build_model(activation_bits, weight_noise_train, weight_noise_test, weights)\n",
    "ckpt_dir = os.path.join(os.getcwd(), model_type)\n",
    "ft_path = os.path.join(ckpt_dir, ft_model_name)\n",
    "checkpoint = ModelCheckpoint(filepath=ft_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "model.fit([train_previous_inputs, train_current_inputs], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=([test_previous_inputs, test_current_inputs], y_test),\n",
    "          shuffle=True)\n",
    "#           callbacks=[checkpoint])\n",
    "\n",
    "model.save(ft_path + '_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_conv2d_noise_9_relu_decay_1e-1_4_final_3/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(ft_path + '_final_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 160 steps, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8884\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.92200, saving model to /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2\n",
      "INFO:tensorflow:Assets written to: /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2/assets\n",
      "160/160 [==============================] - 21s 131ms/step - loss: 0.3228 - accuracy: 0.8885 - val_loss: 0.2416 - val_accuracy: 0.9220\n",
      "Epoch 2/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3274 - accuracy: 0.8861\n",
      "Epoch 00002: val_accuracy did not improve from 0.92200\n",
      "160/160 [==============================] - 5s 30ms/step - loss: 0.3272 - accuracy: 0.8863 - val_loss: 0.2368 - val_accuracy: 0.9213\n",
      "Epoch 3/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3334 - accuracy: 0.8844\n",
      "Epoch 00003: val_accuracy did not improve from 0.92200\n",
      "160/160 [==============================] - 5s 30ms/step - loss: 0.3332 - accuracy: 0.8845 - val_loss: 0.2392 - val_accuracy: 0.9188\n",
      "Epoch 4/20\n",
      "157/160 [============================>.] - ETA: 0s - loss: 0.3273 - accuracy: 0.8866\n",
      "Epoch 00004: val_accuracy improved from 0.92200 to 0.92240, saving model to /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2\n",
      "INFO:tensorflow:Assets written to: /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2/assets\n",
      "160/160 [==============================] - 14s 88ms/step - loss: 0.3282 - accuracy: 0.8859 - val_loss: 0.2354 - val_accuracy: 0.9224\n",
      "Epoch 5/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8891\n",
      "Epoch 00005: val_accuracy did not improve from 0.92240\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3210 - accuracy: 0.8893 - val_loss: 0.2342 - val_accuracy: 0.9220\n",
      "Epoch 6/20\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.8895\n",
      "Epoch 00006: val_accuracy improved from 0.92240 to 0.92310, saving model to /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2\n",
      "INFO:tensorflow:Assets written to: /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2/assets\n",
      "160/160 [==============================] - 15s 91ms/step - loss: 0.3174 - accuracy: 0.8896 - val_loss: 0.2304 - val_accuracy: 0.9231\n",
      "Epoch 7/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8917\n",
      "Epoch 00007: val_accuracy did not improve from 0.92310\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3220 - accuracy: 0.8913 - val_loss: 0.2327 - val_accuracy: 0.9211\n",
      "Epoch 8/20\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.8910 ETA: 1s - l\n",
      "Epoch 00008: val_accuracy improved from 0.92310 to 0.92490, saving model to /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2\n",
      "INFO:tensorflow:Assets written to: /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2/assets\n",
      "160/160 [==============================] - 15s 92ms/step - loss: 0.3120 - accuracy: 0.8911 - val_loss: 0.2303 - val_accuracy: 0.9249\n",
      "Epoch 9/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8909\n",
      "Epoch 00009: val_accuracy did not improve from 0.92490\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3192 - accuracy: 0.8907 - val_loss: 0.2314 - val_accuracy: 0.9224\n",
      "Epoch 10/20\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8889\n",
      "Epoch 00010: val_accuracy did not improve from 0.92490\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3202 - accuracy: 0.8893 - val_loss: 0.2328 - val_accuracy: 0.9212\n",
      "Epoch 11/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3211 - accuracy: 0.8900\n",
      "Epoch 00011: val_accuracy did not improve from 0.92490\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3212 - accuracy: 0.8899 - val_loss: 0.2296 - val_accuracy: 0.9242\n",
      "Epoch 12/20\n",
      "157/160 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.8893\n",
      "Epoch 00012: val_accuracy did not improve from 0.92490\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3216 - accuracy: 0.8893 - val_loss: 0.2249 - val_accuracy: 0.9243\n",
      "Epoch 13/20\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8912 ETA: 1s -\n",
      "Epoch 00013: val_accuracy did not improve from 0.92490\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3188 - accuracy: 0.8911 - val_loss: 0.2276 - val_accuracy: 0.9248\n",
      "Epoch 14/20\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8916\n",
      "Epoch 00014: val_accuracy did not improve from 0.92490\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3108 - accuracy: 0.8915 - val_loss: 0.2316 - val_accuracy: 0.9207\n",
      "Epoch 15/20\n",
      "157/160 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8956\n",
      "Epoch 00015: val_accuracy did not improve from 0.92490\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3102 - accuracy: 0.8957 - val_loss: 0.2275 - val_accuracy: 0.9241\n",
      "Epoch 16/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.8926\n",
      "Epoch 00016: val_accuracy improved from 0.92490 to 0.92550, saving model to /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2\n",
      "INFO:tensorflow:Assets written to: /home/users/weierwan/cifar10_resnet_neurram/ResNet20v1_filter16_act3b_wnoise0.20_input0.95/fwd_finetune_all_val_2/assets\n",
      "160/160 [==============================] - 14s 89ms/step - loss: 0.3141 - accuracy: 0.8926 - val_loss: 0.2279 - val_accuracy: 0.9255\n",
      "Epoch 17/20\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.8916\n",
      "Epoch 00017: val_accuracy did not improve from 0.92550\n",
      "160/160 [==============================] - 5s 30ms/step - loss: 0.3071 - accuracy: 0.8915 - val_loss: 0.2278 - val_accuracy: 0.9241\n",
      "Epoch 18/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3117 - accuracy: 0.8907\n",
      "Epoch 00018: val_accuracy did not improve from 0.92550\n",
      "160/160 [==============================] - 5s 30ms/step - loss: 0.3112 - accuracy: 0.8910 - val_loss: 0.2264 - val_accuracy: 0.9238\n",
      "Epoch 19/20\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8937\n",
      "Epoch 00019: val_accuracy did not improve from 0.92550\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3100 - accuracy: 0.8940 - val_loss: 0.2268 - val_accuracy: 0.9230\n",
      "Epoch 20/20\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.8907 ETA: 0s - loss: 0.3190 \n",
      "Epoch 00020: val_accuracy did not improve from 0.92550\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.3145 - accuracy: 0.8910 - val_loss: 0.2296 - val_accuracy: 0.9226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f98b545c8d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune with feature map augmentation\n",
    "\n",
    "model = build_model(activation_bits, weight_noise_train, weight_noise_test, weights)\n",
    "ckpt_dir = os.path.join(os.getcwd(), model_type)\n",
    "ft_path = os.path.join(ckpt_dir, ft_model_name)\n",
    "checkpoint = ModelCheckpoint(filepath=ft_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "print('Using real-time data augmentation.')\n",
    "\n",
    "data_gen_args = dict(\n",
    "    # set input mean to 0 over the dataset\n",
    "    featurewise_center=False,\n",
    "    # set each sample mean to 0\n",
    "    samplewise_center=False,\n",
    "    # divide inputs by std of dataset\n",
    "    featurewise_std_normalization=False,\n",
    "    # divide each input by its std\n",
    "    samplewise_std_normalization=False,\n",
    "    # apply ZCA whitening\n",
    "    zca_whitening=False,\n",
    "    # epsilon for ZCA whitening\n",
    "    zca_epsilon=1e-06,\n",
    "    # randomly rotate images in the range (deg 0 to 180)\n",
    "    rotation_range=0,\n",
    "    # randomly shift images horizontally\n",
    "    width_shift_range=0.0,\n",
    "    # randomly shift images vertically\n",
    "    height_shift_range=0.0,\n",
    "    # set range for random shear\n",
    "    shear_range=0.,\n",
    "    # set range for random zoom\n",
    "    zoom_range=0.,\n",
    "    # set range for random channel shifts\n",
    "    channel_shift_range=0.,\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    # value used for fill_mode = \"constant\"\n",
    "    cval=0.,\n",
    "    # randomly flip images\n",
    "    horizontal_flip=True,\n",
    "    # randomly flip images\n",
    "    vertical_flip=False,\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "previous_datagen = ImageDataGenerator(**data_gen_args)\n",
    "# current_datagen = ImageDataGenerator(**data_gen_args)\n",
    "seed = 1\n",
    "\n",
    "\n",
    "# def generator_two_img(gen1, gen2, X1, X2, y, batch_size):\n",
    "#     genX1 = gen1.flow(X1, y,  batch_size=batch_size, seed=1)\n",
    "#     genX2 = gen2.flow(X2, y, batch_size=batch_size, seed=1)\n",
    "#     while True:\n",
    "#         X1i = genX1.next()\n",
    "#         X2i = genX2.next()\n",
    "#         yield [X1i[0], X2i[0]], X1i[1]\n",
    "\n",
    "# train_generator = generator_two_img(previous_datagen, previous_datagen, train_previous_inputs_div, train_current_inputs_div, y_train_div, batch_size)\n",
    "train_generator = previous_datagen.flow(train_previous_inputs_div, y_train_div, batch_size=batch_size, seed=seed)\n",
    "\n",
    "# # Fit the model on the batches generated by datagen.flow().\n",
    "model.fit(train_generator, steps_per_epoch = 160,\n",
    "        validation_data=([val_previous_inputs_div], y_val_div),\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 3s 56us/sample - loss: 0.1085 - accuracy: 0.9653\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.5255 - accuracy: 0.8562\n"
     ]
    }
   ],
   "source": [
    "# Score fine-tuned model.\n",
    "ckpt_dir = os.path.join(os.getcwd(), model_type)\n",
    "# ft_model_name = 'fwd_finetune_%s_relu_decay_1e-2_3_final' % finetune_current_layer\n",
    "ft_path = os.path.join(ckpt_dir, ft_model_name + '_final')\n",
    "model = load_model(ft_path)\n",
    "weights_ft = {}\n",
    "for klayer in model.layers:\n",
    "    weights_ft[klayer.name] = klayer.get_weights()\n",
    "\n",
    "accuracy_train_nf = model.evaluate([train_previous_inputs, train_current_inputs], y_train, verbose=1)[1]\n",
    "accuracy_test_nf = model.evaluate([test_previous_inputs, test_current_inputs], y_test, verbose=1)[1]\n",
    "accuracy_val_nf = model.evaluate([val_previous_inputs_div, val_current_inputs_div], y_val_div, verbose=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(activation_bits, weight_noise_train, 0.2, weights_ft)\n",
    "accuracy = np.zeros(10)\n",
    "for i in range(10):\n",
    "    accuracy[i] = model.evaluate([test_previous_inputs, test_current_inputs], y_test, verbose=1)[1]\n",
    "print(accuracy.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
